{
  "name": "RAG Q&A Workflow",
  "nodes": [
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.3,
      "position": [
        -1456,
        32
      ],
      "id": "90cae21f-8b81-492c-8c09-596b4af7f698",
      "name": "When chat message received",
      "webhookId": "c3f535cd-9e49-4888-ac64-2119616253b8"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Original question: {{ $json.chatInput }}",
        "hasOutputParser": true,
        "messages": {
          "messageValues": [
            {
              "message": "=You are an advanced AI assistant. Your task is to analyze the <original_user_question>{{ $json.chatInput }}</original_user_question> and generate 1 to 3 alternative versions (paraphrases) of it.\n\n<role></role>\n<guidelines>\n  <guideline number=\"1\" name=\"focus_on_meaning\">\n    **Paraphrases should retain the original meaning** of the <original_user_question>ORIGINAL USER QUERY</original_user_question>, but they can use synonyms and different sentence structures.\n  </guideline>\n  \n  <guideline number=\"2\" name=\"language\">\n    **All paraphrases must be in English.**\n  </guideline>\n</guidelines>\n\n**Output Format:** \nReturn ONLY a flat JSON object with exactly these keys: \n\"paraphrased_query[1]\", \"paraphrased_query[2]\", \"paraphrased_query[3]\".\nDo NOT include any other keys, metadata, schema, or text.\nDo NOT wrap in \"output\" or any other root object.\nHere is an example of the expected output:\n\n{\n  \"paraphrased_query[1]\": \"What martial arts do you know?\",\n  \"paraphrased_query[2]\": \"Which martial arts are the most popular in the world?\",\n  \"paraphrased_query[3]\": \"Which martial art is the best to start learning?\"\n}"
            }
          ]
        },
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        -1280,
        32
      ],
      "id": "243263f0-98f7-42bf-a39b-abf719c32124",
      "name": "Basic LLM Chain"
    },
    {
      "parameters": {
        "fieldToSplitOut": "output",
        "options": {
          "destinationFieldName": "paraphrased_query"
        }
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        -992,
        32
      ],
      "id": "0932f8fb-3b3a-4cee-bcdc-2a6cb715f595",
      "name": "Split Out"
    },
    {
      "parameters": {
        "mode": "load",
        "pineconeIndex": {
          "__rl": true,
          "value": "n8n-mistral-embeddings",
          "mode": "list",
          "cachedResultName": "n8n-mistral-embeddings"
        },
        "prompt": "={{ $json.paraphrased_query }}",
        "options": {
          "pineconeNamespace": "martial_arts_knowledge_base"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "typeVersion": 1.3,
      "position": [
        -816,
        32
      ],
      "id": "4789b959-6f3d-4dcd-9781-e2a9d8c89327",
      "name": "Pinecone Vector Store",
      "credentials": {
        "pineconeApi": {
          "id": "7eVvFuI6bRQc7QZh",
          "name": "PineconeApi account"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsMistralCloud",
      "typeVersion": 1,
      "position": [
        -784,
        224
      ],
      "id": "a6943347-fc50-46d3-8ae3-264d056d41cb",
      "name": "Embeddings Mistral Cloud",
      "credentials": {
        "mistralCloudApi": {
          "id": "38NwFL7hYNRXsa8K",
          "name": "Mistral Cloud account 2"
        }
      }
    },
    {
      "parameters": {
        "model": "open-mistral-7b",
        "options": {
          "maxTokens": 256,
          "temperature": 0
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatMistralCloud",
      "typeVersion": 1,
      "position": [
        -1328,
        224
      ],
      "id": "49b97832-76e3-4e78-a50c-aab0af4f3cdd",
      "name": "Mistral Cloud Chat Model",
      "credentials": {
        "mistralCloudApi": {
          "id": "38NwFL7hYNRXsa8K",
          "name": "Mistral Cloud account 2"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "800e398b-872c-49e7-8b08-a0822f91632b",
              "leftValue": "={{ $json.score }}",
              "rightValue": 0.4,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        -544,
        32
      ],
      "id": "043bf768-934a-403b-8d6d-19e803c04066",
      "name": "Filter"
    },
    {
      "parameters": {
        "compare": "selectedFields",
        "fieldsToCompare": "document.pageContent",
        "options": {}
      },
      "type": "n8n-nodes-base.removeDuplicates",
      "typeVersion": 2,
      "position": [
        -368,
        32
      ],
      "id": "a202e2a2-5e29-4950-9053-8f1f6cf5c23d",
      "name": "Remove Duplicates"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=User question: {{ $('When chat message received').item.json.chatInput }}\n\nVerified knowledge: {{ $json.pageContent }}",
        "messages": {
          "messageValues": [
            {
              "message": "=## Role\nYou are an advanced AI assistant.  \nYour primary task is to provide a precise and concise answer to the <user_question>USER QUESTION</user_question>, based SOLELY on the information contained within the provided <knowledge_context>CONTEXT</knowledge_context>.  \nThis context originates from a searched knowledge base (e.g., documents or transcripts) in response to the user’s question.\n\n---------------------------------------------\nOPERATING RULES (ROLE & GUIDELINES)\n---------------------------------------------\n\n1. ADHERENCE TO CONTEXT  \nRespond strictly based on the information found in the <knowledge_context>CONTEXT</knowledge_context>.  \nYou are not allowed to include any external knowledge beyond this context — even if you personally know the correct answer.\n\n2. INFORMATION IDENTIFICATION  \nCarefully analyze the <knowledge_context>CONTEXT</knowledge_context> in relation to the <user_question>USER QUESTION</user_question>.  \nIdentify the parts of the context that directly and comprehensively address the user’s question.\n\n3. PRECISION AND CONCISENESS  \nFormulate your response clearly, directly, and as concisely as possible.  \nHowever, ensure that your answer includes all relevant information from the <knowledge_context>CONTEXT</knowledge_context> necessary for a complete response.\n\n4. LACK OF INFORMATION (NO ANSWER PROTOCOL)  \nIf the provided <knowledge_context>CONTEXT</knowledge_context> does not contain sufficient data, your answer must be:  \n\"I'm sorry, but I couldn’t find any information about this topic in the provided knowledge base excerpts.\"  \nIn such cases, do not attempt to guess or fabricate any information.\n\n5. OPTIONAL SOURCE REFERENCE STYLE  \nIf appropriate, you may subtly indicate that your response is based on the provided materials, e.g.:  \n\"According to the provided information...\" or  \n\"In the knowledge base, it is mentioned that...\".  \nAvoid excessive formality and do not quote verbatim unless it’s essential for clarity or accuracy.\n\n6. LANGUAGE AND CLARITY  \nThe response should be written in natural, conversational English.  \nAvoid technical jargon unless it appears in the <knowledge_context>CONTEXT</knowledge_context> and is crucial for understanding — in that case, briefly explain it.\n\n7. TONE OF RESPONSE  \nMaintain a professional yet approachable and helpful tone.  \nYour communication should be clear, confident, and informative; you may include a touch of appropriate lightness, but the focus must remain on delivering value.\n\n---------------------------------------------\nINPUT DATA FORMAT\n---------------------------------------------\n\nInput data will be provided in the following structure:\n\n<knowledge_context>\n{{ $json.pageContent }}\n</knowledge_context>\n\n<user_question>\n{{ $('When chat message received').item.json.chatInput }}\n</user_question>\n\n---------------------------------------------\nSYSTEM INSTRUCTION\n---------------------------------------------\n\nYour task is to generate only the <final_answer>ANSWER</final_answer> based on the given data.\nReturn the answer as **plain text only**.\n\n\n8. OUTPUT FORMAT\nReturn only the plain text of the answer. Do not include any XML/HTML tags like <final_answer>."
            }
          ]
        },
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        -32,
        32
      ],
      "id": "517469fa-18a4-4347-aceb-cde7bb830286",
      "name": "Basic LLM Chain1"
    },
    {
      "parameters": {
        "content": "Triggers the workflow whenever a new chat message is received via the connected chat interface.",
        "height": 256,
        "width": 176
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1488,
        -80
      ],
      "typeVersion": 1,
      "id": "3ed27f71-9802-42d2-8f19-e315fab77124",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "content": "Generates 1–3 paraphrased versions of the user’s question using the LLM, based on system instructions.",
        "height": 208,
        "color": 3
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1296,
        -64
      ],
      "typeVersion": 1,
      "id": "d722f290-a572-4fbf-a1ce-7b85451eadfa",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "content": "\n\n\n\n\n\n\n\n\n\n\nParses the LLM output into a structured JSON format for easier handling in the workflow.",
        "height": 224,
        "width": 208,
        "color": 5
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1168,
        192
      ],
      "typeVersion": 1,
      "id": "0aec1b30-f389-422a-af88-e51c49095f3d",
      "name": "Sticky Note2"
    },
    {
      "parameters": {
        "content": "Splits the parsed paraphrased queries into individual items so each can be processed separately.",
        "height": 240,
        "width": 176,
        "color": 4
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1024,
        -80
      ],
      "typeVersion": 1,
      "id": "b2c03f47-91f1-4295-bbf8-322f1f9539f5",
      "name": "Sticky Note3"
    },
    {
      "parameters": {
        "content": "Retrieves relevant knowledge from the Pinecone vector database based on each paraphrased query.",
        "height": 208,
        "width": 256,
        "color": 5
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -832,
        -64
      ],
      "typeVersion": 1,
      "id": "25a9fdf4-0517-43f8-80df-d0160d0373ca",
      "name": "Sticky Note4"
    },
    {
      "parameters": {
        "content": "\n\n\n\n\n\n\n\n\n\n\nGenerates embeddings for queries if needed for vector search in Pinecone.",
        "height": 208,
        "width": 208,
        "color": 6
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -832,
        208
      ],
      "typeVersion": 1,
      "id": "4339d056-9551-4c3a-9704-26d88e9fc615",
      "name": "Sticky Note5"
    },
    {
      "parameters": {
        "content": "\n\n\n\n\n\n\n\n\n\n\n\nProcesses each paraphrased query through the LLM to prepare for retrieval or further processing.",
        "height": 256,
        "width": 208,
        "color": 4
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1392,
        192
      ],
      "typeVersion": 1,
      "id": "8cae6917-013f-4813-ba6f-5e1d1f790f31",
      "name": "Sticky Note6"
    },
    {
      "parameters": {
        "content": "Filters out retrieved documents with a similarity score below 0.4 to ensure relevance.",
        "height": 256,
        "width": 150,
        "color": 6
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -560,
        -96
      ],
      "typeVersion": 1,
      "id": "aa067941-05bf-417b-9852-fbdaabe795be",
      "name": "Sticky Note7"
    },
    {
      "parameters": {
        "content": "Removes duplicate documents to avoid redundant information.",
        "height": 224,
        "width": 160,
        "color": 7
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -400,
        -64
      ],
      "typeVersion": 1,
      "id": "c7e83453-c78e-4a53-b84b-74717b85d6dc",
      "name": "Sticky Note8"
    },
    {
      "parameters": {
        "content": "Aggregates the filtered and deduplicated documents into a single content block to feed the final answer generation.",
        "height": 288,
        "width": 176,
        "color": 2
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -224,
        -112
      ],
      "typeVersion": 1,
      "id": "20336f07-0e0d-4ef9-b6d1-8d97ace42e46",
      "name": "Sticky Note9"
    },
    {
      "parameters": {
        "content": "Generates the final answer to the user’s question using the aggregated knowledge. Follows strict guidelines for context, clarity, and tone. Ensures output is plain text only.",
        "height": 240,
        "color": 3
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -32,
        -96
      ],
      "typeVersion": 1,
      "id": "407550a5-eaa6-4b88-987f-55172d57938e",
      "name": "Sticky Note10"
    },
    {
      "parameters": {
        "content": "\n\n\n\n\n\n\n\n\n\n\nRuns the final LLM model to produce the answer based on the prepared prompt and retrieved knowledge.",
        "height": 224,
        "color": 7
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -96,
        192
      ],
      "typeVersion": 1,
      "id": "6c2e72fd-aa71-4649-85e3-24a392dfba44",
      "name": "Sticky Note11"
    },
    {
      "parameters": {
        "model": "devstral-small-latest",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatMistralCloud",
      "typeVersion": 1,
      "position": [
        -32,
        208
      ],
      "id": "3900e429-f144-4c06-8f78-bfc7c6e58504",
      "name": "Mistral Cloud Chat Model2",
      "credentials": {
        "mistralCloudApi": {
          "id": "38NwFL7hYNRXsa8K",
          "name": "Mistral Cloud account 2"
        }
      }
    },
    {
      "parameters": {
        "fieldsToAggregate": {
          "fieldToAggregate": [
            {
              "fieldToAggregate": "document.pageContent"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        -176,
        32
      ],
      "id": "f2a4f779-227e-4dc5-9e55-5d565fbd8b03",
      "name": "Aggregate"
    },
    {
      "parameters": {
        "jsonSchemaExample": "{\n  \"paraphrased_query[1]\": \"Example paraphrase 1.\",\n  \"paraphrased_query[2]\": \"Example paraphrase 2.\",\n  \"paraphrased_query[3]\": \"Example paraphrase 3.\"\n}"
      },
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "typeVersion": 1.3,
      "position": [
        -1120,
        208
      ],
      "id": "cf466158-1116-4871-885e-13d7ae8c7339",
      "name": "Structured Output Parser"
    }
  ],
  "pinData": {},
  "connections": {
    "When chat message received": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "Split Out",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Out": {
      "main": [
        [
          {
            "node": "Pinecone Vector Store",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings Mistral Cloud": {
      "ai_embedding": [
        [
          {
            "node": "Pinecone Vector Store",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Mistral Cloud Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Pinecone Vector Store": {
      "main": [
        [
          {
            "node": "Filter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Filter": {
      "main": [
        [
          {
            "node": "Remove Duplicates",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Remove Duplicates": {
      "main": [
        [
          {
            "node": "Aggregate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Mistral Cloud Chat Model2": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate": {
      "main": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "b243897e-aa5b-4209-b846-b57c32c057e7",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "4de8a13b1a92f934a2c777a93d52eceb8549afda747f6c680f227b748b6c36da"
  },
  "id": "isWAlxS6APYkegmE",
  "tags": []
}